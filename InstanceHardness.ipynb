{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gradually adds different complexity metrics to the ledger of all predictions across DVs, models, epochs -- essentially left-joining new metrics onto the ledger.\n",
    "\n",
    "We checkpoint several of these intermediate ledgers, but the final output of this notebook is ledger_len, which is passed to ArtifactProcessing.ipynb to create input data for the artifacts (i.e., tables and figures) included in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stored values can be loaded at several checkpoints by setting the `USED_STORED` flag to True, else the notebook will recalculate these values.  Initial data loading can take 8-10min but most other calculations finish in 1-2min with parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_STORED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ledgers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store \"ledgers'\" for each model trained, where the ledger tracks model output / loss for every instance over each epoch.\n",
    "They are inherently heavy documents (w.r.t. memory) but we need this instance-epoch level of granularity to calculate dynamic complexity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.post_processing import *\n",
    "\n",
    "if USE_STORED:\n",
    "    # WARNING: takes ~20sec\n",
    "    # TODO; replace with 3rd part hosting\n",
    "    all_ledgers = pd.read_csv( '/Users/ryancook/Downloads/all_ledgers.csv', keep_default_na=False, low_memory=False )\n",
    "    PROBLEM_ID = 'f5_r594'\n",
    "    all_ledgers = remove_duplicate_id( PROBLEM_ID, all_ledgers )\n",
    "else:\n",
    "    # WARNING: takes ~8min\n",
    "    # TODO; better time printout\n",
    "    from utils.post_processing import load_and_clean_ledgers\n",
    "    all_ledgers = load_and_clean_ledgers()\n",
    "\n",
    "# used later to ensure proper joins\n",
    "check_counts = all_ledgers.groupby([ 'score_var', 'split_full' ])[ 'ID' ].count().reset_index().rename(columns={'ID': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v-Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty of a given instance can be seen as its lack of v-usable information, which considers the accessibility of Shannon mutual information between an encrypted input $X$ and an output $Y$ ()Xu et al., 2020; Shannon, 2001; Ethayarajh et al., 2022).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$\n",
    "    \\text{PVI}(x_i \\rightarrow y_i) = -\\log_2 H_{\\nu}(Y) +\n",
    "    \\log_2 H_{\\nu}(Y|X)\n",
    "$\n",
    "\n",
    "\n",
    "where $H_{\\nu}(y_i) = \\mathbf{E}[-\\log g(y_i | \\varnothing)]$ for ``null model'' $g$ trained on null string $\\varnothing$ and $H_{\\nu}(y_i) = \\mathbf{E}[-\\log g'(y_i | x_i)]$ from primary model $g'$.\n",
    "\n",
    "<br>\n",
    "\n",
    "These calculations can be done below in ~2sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probs):\n",
    "    return -1 * np.log2(probs)\n",
    "\n",
    "def calculate_pvi(model_probs, null_probs):\n",
    "    return calculate_entropy( null_probs ) - calculate_entropy( model_probs )\n",
    "\n",
    "\n",
    "# Fast row-wise calculation since don't need to go by epoch\n",
    "all_ledgers['null_probs'] = np.where(all_ledgers['null_probs']=='', np.NaN, all_ledgers['null_probs'] )\n",
    "all_ledgers['PVI'] = calculate_pvi(all_ledgers['probs'].astype(float), all_ledgers['null_probs'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forgetting Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forgotten examples are instances which are classified correctly in earlier epochs but misclassified at some later epoch in the training process (Toneva et al., 2019).  These instances that are more frequently forgotten are considered to be more complex.\n",
    "\n",
    "<br>\n",
    "\n",
    "$\n",
    "    \\text{TF} = \\sum \\limits_{e=1}^{|E|} \\sum \\limits_{k=e+1}^{|E|} \\mathbf{f}(y_i | x_i)\n",
    "$\n",
    "\n",
    "where $e \\in E$ are training epochs.\n",
    "\n",
    "<br>\n",
    "\n",
    "These calculations can be done below in ~2min with parallelization.\n",
    "Note that all parallelization was done on $n-1=7$ cores of a Mac M1 machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember returns zero-indexed IDXs not EPOCHs\n",
    "def get_learn_forget_events(corr_idxs, last_idx=14):\n",
    "    if len(corr_idxs)==0:\n",
    "        return [], []\n",
    "    \n",
    "    # first one is always a learning event\n",
    "    these_les = [corr_idxs[0]]\n",
    "    these_fes = []\n",
    "    # loop the rest of them\n",
    "    counter = 1\n",
    "    for cid in corr_idxs[1:]:\n",
    "        prev_cid = corr_idxs[counter-1]\n",
    "        counter+=1\n",
    "\n",
    "        # a break of >1 cdx implies a learning event\n",
    "        if prev_cid+1 == cid:\n",
    "            continue\n",
    "        else:\n",
    "            these_les.append(cid)\n",
    "            # NOTE; we also have a forgetting event after last cid\n",
    "            these_fes.append(prev_cid+1)\n",
    "\n",
    "    # check if we forget it again after cids\n",
    "    last_cid = corr_idxs[-1]\n",
    "    if last_cid!=last_idx:\n",
    "        these_fes.append(last_cid+1)\n",
    "    \n",
    "    return these_les, these_fes\n",
    "\n",
    "\n",
    "def proc_mn_forget(mn):\n",
    "    mdf = all_ledgers[all_ledgers['model_name']==mn]\n",
    "    # forgetting stats done at model level\n",
    "    corr_by_ep = mdf.pivot_table('correct', ['ID'], 'epoch')\n",
    "\n",
    "    idf = pd.DataFrame()\n",
    "    for item in corr_by_ep.index:\n",
    "        this_row = corr_by_ep.loc[item, :]\n",
    "        corr_idxs = list(np.where(np.array(this_row)==1)[0])\n",
    "\n",
    "        learn_events, forget_events = get_learn_forget_events(corr_idxs)\n",
    "        this_d = {'ID': item,\n",
    "                  'ep_times_learned': len(learn_events),\n",
    "                  'ep_times_forgotten': len(forget_events),\n",
    "                  'ep_is_unforgettable': int( len(forget_events)==0 )}\n",
    "        idf = pd.concat([ idf, pd.DataFrame.from_dict([ this_d ]) ])\n",
    "\n",
    "    return pd.merge( mdf, idf, on='ID', how='left' )\n",
    "\n",
    "\n",
    "USE_STORED = False\n",
    "\n",
    "if USE_STORED:\n",
    "    ledger_forget = pd.read_csv( '/Users/ryancook/Downloads/these_ledgers/ledger_forget.csv', keep_default_na=False )\n",
    "    ledger_forget['split_full'] = [ s + '-' + ledger_forget['strat'].iloc[sdx] if s=='train' else s for sdx, s in enumerate(ledger_forget['split']) ]\n",
    "else:\n",
    "    # start with index-level accuracy (i.e., correct)\n",
    "    all_ledgers['correct'] = (all_ledgers['preds'] == all_ledgers['labels']).astype(int)\n",
    "        \n",
    "    #process files in parallel\n",
    "    import multiprocess\n",
    "    #multisetup\n",
    "    cores_to_use = multiprocess.cpu_count()-1\n",
    "    pool = multiprocess.Pool(cores_to_use)\n",
    "\n",
    "    # WARNING: takes ~2min\n",
    "    all_mns = all_ledgers['model_name'].unique()\n",
    "    with multiprocess.Pool(cores_to_use) as pool:\n",
    "        out_list = pool.map(proc_mn_forget, all_mns)\n",
    "\n",
    "    ledger_forget = pd.concat( out_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>epoch</th>\n",
       "      <th>split</th>\n",
       "      <th>ID</th>\n",
       "      <th>labels</th>\n",
       "      <th>probs</th>\n",
       "      <th>preds</th>\n",
       "      <th>losses</th>\n",
       "      <th>null_probs</th>\n",
       "      <th>score_var</th>\n",
       "      <th>strat</th>\n",
       "      <th>split_full</th>\n",
       "      <th>PVI</th>\n",
       "      <th>correct</th>\n",
       "      <th>ep_times_learned</th>\n",
       "      <th>ep_times_forgotten</th>\n",
       "      <th>ep_is_unforgettable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>local-ffn_Numeracy_ft-True_sub-False_t7_numLay...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>f4_r656</td>\n",
       "      <td>1</td>\n",
       "      <td>0.452078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793901</td>\n",
       "      <td>0.49707943</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>train-Constant</td>\n",
       "      <td>-0.136905</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local-ffn_Numeracy_ft-True_sub-False_t7_numLay...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>f5_r523</td>\n",
       "      <td>0</td>\n",
       "      <td>0.494448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682105</td>\n",
       "      <td>0.49707943</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>train-Constant</td>\n",
       "      <td>-0.007656</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>local-ffn_Numeracy_ft-True_sub-False_t7_numLay...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>f5_r402</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810704</td>\n",
       "      <td>0.49707943</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>train-Constant</td>\n",
       "      <td>-0.161148</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>local-ffn_Numeracy_ft-True_sub-False_t7_numLay...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>f4_r855</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765048</td>\n",
       "      <td>0.49707943</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>train-Constant</td>\n",
       "      <td>-0.095279</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>local-ffn_Numeracy_ft-True_sub-False_t7_numLay...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>f2_r1164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.439030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578088</td>\n",
       "      <td>0.49707943</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>train-Constant</td>\n",
       "      <td>-0.179157</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79855</th>\n",
       "      <td>local-cnn_Numeracy_ft-True_sub-False_t5_numLay...</td>\n",
       "      <td>15</td>\n",
       "      <td>test</td>\n",
       "      <td>f1_r1697</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.463550</td>\n",
       "      <td>0.5009064674377441</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.433304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79856</th>\n",
       "      <td>local-cnn_Numeracy_ft-True_sub-False_t5_numLay...</td>\n",
       "      <td>15</td>\n",
       "      <td>test</td>\n",
       "      <td>f1_r1698</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437333</td>\n",
       "      <td>0.5009064674377441</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.499800</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79857</th>\n",
       "      <td>local-cnn_Numeracy_ft-True_sub-False_t5_numLay...</td>\n",
       "      <td>15</td>\n",
       "      <td>test</td>\n",
       "      <td>f1_r1699</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436136</td>\n",
       "      <td>0.5009064674377441</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.502953</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79858</th>\n",
       "      <td>local-cnn_Numeracy_ft-True_sub-False_t5_numLay...</td>\n",
       "      <td>15</td>\n",
       "      <td>test</td>\n",
       "      <td>f1_r1700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448153</td>\n",
       "      <td>0.5009064674377441</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.471770</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79859</th>\n",
       "      <td>local-cnn_Numeracy_ft-True_sub-False_t5_numLay...</td>\n",
       "      <td>15</td>\n",
       "      <td>test</td>\n",
       "      <td>f1_r1701</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.039956</td>\n",
       "      <td>0.5009064674377441</td>\n",
       "      <td>Numeracy</td>\n",
       "      <td>Constant</td>\n",
       "      <td>test</td>\n",
       "      <td>-0.502953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15504046 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              model_name  epoch  split  \\\n",
       "0      local-ffn_Numeracy_ft-True_sub-False_t7_numLay...      1  train   \n",
       "1      local-ffn_Numeracy_ft-True_sub-False_t7_numLay...      1  train   \n",
       "2      local-ffn_Numeracy_ft-True_sub-False_t7_numLay...      1  train   \n",
       "3      local-ffn_Numeracy_ft-True_sub-False_t7_numLay...      1  train   \n",
       "4      local-ffn_Numeracy_ft-True_sub-False_t7_numLay...      1  train   \n",
       "...                                                  ...    ...    ...   \n",
       "79855  local-cnn_Numeracy_ft-True_sub-False_t5_numLay...     15   test   \n",
       "79856  local-cnn_Numeracy_ft-True_sub-False_t5_numLay...     15   test   \n",
       "79857  local-cnn_Numeracy_ft-True_sub-False_t5_numLay...     15   test   \n",
       "79858  local-cnn_Numeracy_ft-True_sub-False_t5_numLay...     15   test   \n",
       "79859  local-cnn_Numeracy_ft-True_sub-False_t5_numLay...     15   test   \n",
       "\n",
       "             ID  labels     probs  preds    losses          null_probs  \\\n",
       "0       f4_r656       1  0.452078    0.0  0.793901          0.49707943   \n",
       "1       f5_r523       0  0.494448    0.0  0.682105          0.49707943   \n",
       "2       f5_r402       1  0.444545    0.0  0.810704          0.49707943   \n",
       "3       f4_r855       1  0.465312    0.0  0.765048          0.49707943   \n",
       "4      f2_r1164       0  0.439030    0.0  0.578088          0.49707943   \n",
       "...         ...     ...       ...    ...       ...                 ...   \n",
       "79855  f1_r1697       0  0.370953    0.0  0.463550  0.5009064674377441   \n",
       "79856  f1_r1698       0  0.354244    0.0  0.437333  0.5009064674377441   \n",
       "79857  f1_r1699       0  0.353470    0.0  0.436136  0.5009064674377441   \n",
       "79858  f1_r1700       0  0.361193    0.0  0.448153  0.5009064674377441   \n",
       "79859  f1_r1701       1  0.353470    0.0  1.039956  0.5009064674377441   \n",
       "\n",
       "      score_var     strat      split_full       PVI  correct  \\\n",
       "0      Numeracy  Constant  train-Constant -0.136905        0   \n",
       "1      Numeracy  Constant  train-Constant -0.007656        1   \n",
       "2      Numeracy  Constant  train-Constant -0.161148        0   \n",
       "3      Numeracy  Constant  train-Constant -0.095279        0   \n",
       "4      Numeracy  Constant  train-Constant -0.179157        1   \n",
       "...         ...       ...             ...       ...      ...   \n",
       "79855  Numeracy  Constant            test -0.433304        1   \n",
       "79856  Numeracy  Constant            test -0.499800        1   \n",
       "79857  Numeracy  Constant            test -0.502953        1   \n",
       "79858  Numeracy  Constant            test -0.471770        1   \n",
       "79859  Numeracy  Constant            test -0.502953        0   \n",
       "\n",
       "       ep_times_learned  ep_times_forgotten  ep_is_unforgettable  \n",
       "0                     1                   0                    1  \n",
       "1                     2                   2                    0  \n",
       "2                     1                   1                    0  \n",
       "3                     1                   1                    0  \n",
       "4                     1                   0                    1  \n",
       "...                 ...                 ...                  ...  \n",
       "79855                 1                   0                    1  \n",
       "79856                 1                   0                    1  \n",
       "79857                 1                   0                    1  \n",
       "79858                 1                   0                    1  \n",
       "79859                 0                   0                    1  \n",
       "\n",
       "[15504046 rows x 17 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ledger_forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyHard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyHard algorithm uses instance space analysis to sample only informative meta-features and efficiently generate a single output probability of misclassification (PH) from a pool of seven diverse classifiers (Paiva et al., 2021).\n",
    "A higher PH value indicates that an instance has a higher probability of being misclassified.\n",
    "\n",
    "<br>\n",
    "\n",
    "$\n",
    "    \\text{PH}_{\\mathcal{L}} \\Big( \\langle x_i, y_i \\rangle \\Big) = 1 - \\frac{1}{|\\mathcal{L}|} \\sum \\limits_{j=1}^{|\\mathcal{L}|} p \\Big( y_i | x_i, ~g_j(t, \\alpha) \\Big)\n",
    "$\n",
    "\n",
    "for diverse classifiers $\\mathcal{L}$ and $g_j(t, \\alpha)$ is the complete set of learning algorithms and their hyperparameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that these calculations are generated from the <i>run_pyhard.py</i> script and stored in <i>pyhard/</i> for convenient access later.\n",
    "While calculations took several hours on an HTC Condor compute system, they can be added back to the central ledger below in ~1min with parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_hard_sv_split(in_tup):\n",
    "    ph_base = 'pyhard/'\n",
    "\n",
    "    svar, full_split = in_tup\n",
    "\n",
    "    if svar == 'wer':\n",
    "            input_data_d = load_zda_data('data/data_zda/', subset=False)\n",
    "    else:\n",
    "        input_data_d = load_hal_data('data/DataCVFolds/', score_variable=svar, subset=False)\n",
    "    \n",
    "    split = full_split.split('-')[0]\n",
    "\n",
    "    if 'train' in full_split:\n",
    "        input_data_d['hard_train_data'], input_data_d['rand_train_data'] = sample_hard_rand(input_data_d['train_data'], svar)\n",
    "\n",
    "    split_path = full_split if 'train' not in full_split else ('hard_train' if 'Constant' in full_split else 'rand_train' )\n",
    "    this_input_data = input_data_d[split_path+'_data']\n",
    "\n",
    "    full_path = ph_base + f'{svar}/{split_path}/'\n",
    "    this_ih = pd.read_csv( full_path + 'ih.csv' )\n",
    "\n",
    "    ih_id = pd.concat([ this_input_data[['ID']], this_ih ], axis=1)\n",
    "    ih_id['score_var'] = svar\n",
    "    ih_id['split'] = split\n",
    "    ih_id['split_full'] = full_split\n",
    "    ih_id['instance_hardness'] = ih_id['instance_hardness'].fillna(0)\n",
    "\n",
    "    # INIT CHECK: loaded data must match length of stored pyhard \n",
    "    ## stored from previously running run_pyhard.py\n",
    "    if len(this_input_data) != len(ih_id):\n",
    "        print('-----------------------------')\n",
    "        print(f'INIT PROBLEM WITH {svar}-{full_split}')\n",
    "        print('-----------------------------')\n",
    "        raise ValueError('Shape of input data must match instance hardness calculations')\n",
    "\n",
    "    # append pyhard metrics on left-hand side and filter \n",
    "    sub_ledg = ledger_forget[((ledger_forget['score_var']==svar) &  (ledger_forget['split_full']==full_split))]\n",
    "    this_join = pd.merge( sub_ledg, ih_id, on=['score_var', 'split_full', 'split', 'ID'],\n",
    "                            how='left' ).drop('instances', axis=1)\n",
    "    sub_join = this_join[this_join['instance_hardness'].isna()==False]\n",
    "\n",
    "\n",
    "    # JOIN CHECK: must have the same number of IDs in the existing ledger for this task-split\n",
    "    join_check_num = check_counts[( (check_counts['score_var']==svar) &\n",
    "                                (check_counts['split_full']==full_split) )]['count'].iloc[0]\n",
    "    if join_check_num != len(sub_join):\n",
    "        print('-----------------------------')\n",
    "        print(f'JOIN PROBLEM WITH {svar}-{full_split}')\n",
    "        print('-----------------------------')\n",
    "        raise ValueError('Shape of joined data must match expected dimensions of ledger_forget')\n",
    "\n",
    "    return sub_join\n",
    "\n",
    "\n",
    "\n",
    "USE_STORED = False\n",
    "\n",
    "if USE_STORED:\n",
    "    ledger_hardness = pd.read_csv( '/Users/ryancook/Downloads/these_ledgers/ledger_hardness.csv', keep_default_na=False )\n",
    "else:\n",
    "    # NOTE; these are older functions to avoid filtering NAs from demographic information\n",
    "    ## we are only interested in IDs / text strings here\n",
    "    from utils.data_loading_no_demo import *\n",
    "    from utils.data_processing import *\n",
    "        \n",
    "    #process files in parallel\n",
    "    import multiprocess\n",
    "    #multisetup\n",
    "    cores_to_use = multiprocess.cpu_count()-1\n",
    "    pool = multiprocess.Pool(cores_to_use)\n",
    "\n",
    "    from itertools import product\n",
    "    these_svars, these_full_splits = ledger_forget['score_var'].unique(), ledger_forget['split_full'].unique()\n",
    "    all_sv_splits = list( product( these_svars, these_full_splits ) )\n",
    "    \n",
    "    with multiprocess.Pool(cores_to_use) as pool:\n",
    "        out_list = pool.map(proc_hard_sv_split, all_sv_splits)\n",
    "\n",
    "    ledger_hardness = pd.concat( out_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  difficulty of a given item can be considered as the point on the ability scale $\\theta$ where the probability of any subject providing a correct answer is $p(\\theta)=0.5$\n",
    "We can obtain IRT difficulty estimates in a one parameter model by optimizing the Item Characteristic Curve:\n",
    "\n",
    "$\n",
    "    p(\\theta) = \\frac{ \\displaystyle 1}{ \\displaystyle 1 + e^{\\theta-b}}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that calculations were implemented in Working_py-irt.ipynb (in ~2min) since py-irt has conflicting package requirements.\n",
    "Code is included to create train predictions -- since the training ledger only stores val and test split predictions, but we can load these post hoc computed values from stored values in <i>py-irt/<i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/prwzx0bj5_348vgrl_yv9c9w0000gn/T/ipykernel_15318/311227544.py:46: DtypeWarning: Columns (8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ledger_diff = pd.read_csv( '/Users/ryancook/Downloads/these_ledgers/ledger_diff.csv', keep_default_na=False )\n"
     ]
    }
   ],
   "source": [
    "def proc_irt_sv_split(in_tup):\n",
    "    svar, full_split = in_tup\n",
    "    this_id_diff = pd.read_csv( f\"py-irt/{svar}_{full_split}_id_difficulty.csv\" )\n",
    "    this_id_diff['split_full'] = this_id_diff['split']\n",
    "    this_id_diff['split'] = [ s.split('-')[0] for s in this_id_diff['split_full'] ]\n",
    "\n",
    "    sub_ledg = ledger_hardness[((ledger_hardness['score_var']==svar) &  (ledger_hardness['split_full']==full_split))]\n",
    "    this_join = pd.merge( sub_ledg, this_id_diff,\n",
    "                        on=['score_var', 'split_full', 'split', 'ID'], how='left' )\n",
    "    sub_join = this_join[this_join['irt_difficulty'].isna()==False]\n",
    "\n",
    "    # also add boundary proximity while we're looping thru\n",
    "    tdf = get_full_split_df(svar, full_split)\n",
    "    sub_tdf = tdf[['ID', 'score']]\n",
    "    sub_tdf.loc[:, 'split_full'] = full_split\n",
    "    sub_tdf.loc[:, 'score_var'] = svar\n",
    "    sub_tdf.loc[:, 'split'] = [ s.split('-')[0] for s in sub_tdf['split_full'] ]\n",
    "    \n",
    "    bound_join = pd.merge( sub_join, sub_tdf, on=['score_var', 'split_full', 'split', 'ID'], how='left')\n",
    "    bound_sub = bound_join[ bound_join['score'].isna()==False ]\n",
    "\n",
    "    join_check_num = check_counts[( (check_counts['score_var']==svar) & (check_counts['split_full']==full_split) )]['count'].iloc[0]\n",
    "    if join_check_num != len(bound_sub):\n",
    "        print('-----------------------------')\n",
    "        print(f'JOIN PROBLEM WITH {svar}-{full_split}')\n",
    "        print('-----------------------------')\n",
    "        raise ValueError('Shape of joined data must match expected dimensions of ledger_hardness')\n",
    "    # print('passed join check...')\n",
    "\n",
    "    return bound_sub\n",
    "\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "ledger_diff = pd.DataFrame()\n",
    "\n",
    "# NOTE; our splits are named differently\n",
    "split_lst = ['train-None', 'train-Constant', 'val', 'test']\n",
    "check_counts = ledger_hardness.groupby([ 'score_var', 'split_full' ])[ 'ID' ].count().reset_index().rename(columns={'ID': 'count'})\n",
    "\n",
    "\n",
    "USE_STORED = True\n",
    "\n",
    "if USE_STORED:\n",
    "    ledger_diff = pd.read_csv( '/Users/ryancook/Downloads/these_ledgers/ledger_diff.csv', keep_default_na=False )\n",
    "else:\n",
    "    # NOTE; these are older functions to avoid filtering NAs from demographic information\n",
    "    ## we are only interested in IDs / text strings here\n",
    "    from utils.data_loading_no_demo import *\n",
    "    from utils.data_processing import *\n",
    "        \n",
    "    #process files in parallel\n",
    "    import multiprocess\n",
    "    #multisetup\n",
    "    cores_to_use = multiprocess.cpu_count()-1\n",
    "    pool = multiprocess.Pool(cores_to_use)\n",
    "\n",
    "    from itertools import product\n",
    "    these_svars, these_full_splits = ledger_hardness['score_var'].unique(), ledger_hardness['split_full'].unique()\n",
    "    all_sv_splits = list( product( these_svars, these_full_splits ) )\n",
    "    \n",
    "    with multiprocess.Pool(cores_to_use) as pool:\n",
    "        out_list = pool.map(proc_irt_sv_split, all_sv_splits)\n",
    "\n",
    "    ledger_diff = pd.concat( out_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boundary proximity can be seen as \"the difficulty in separating the data points into their expected classes,\" and this complexity increases as the distance between a given point and the classification boundary shrinks (Lorena et al., 2024).\n",
    "\n",
    "$\n",
    "    \\text{BP}(y_{i,c}) = | y_c^* - y_{i,c} |\n",
    "$\n",
    "\n",
    "where $y_c^*$ refers to the class boundary between class $c \\in C$ and its nearest neighboring class $c^*$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can calculate BP from the latent variable score below in ~1min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add boundary proximity\n",
    "# WARNING; takes ~1min\n",
    "med_d = ledger_diff.groupby('score_var')['score'].median().to_dict()\n",
    "ledger_diff['boundary_prox'] = [ abs( score - med_d[ ledger_diff['score_var'].iloc[sdx] ] ) for sdx, score  in enumerate(ledger_diff['score']) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient and popular linguistic heuristic is sentence length, which is a simple count of the number of tokens in the input sequence.\n",
    "As sentence length grows, the sequence becomes more complex as the exponentially increasing number of possibilities makes the class harder to guess by chance (Spitkovsky et al., 2009).\n",
    "\n",
    "$\n",
    "\\text{SL}(x_i) = ~||x_i||\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "We calculate sentence length in the code below in ~1min with parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_sl_sv_split(in_tup):\n",
    "    svar, full_split = in_tup\n",
    "\n",
    "    if svar == 'wer':\n",
    "        input_data_d = load_zda_data('data/data_zda/', subset=False)\n",
    "    else:\n",
    "        input_data_d = load_hal_data('data/DataCVFolds/', score_variable=svar, subset=False)\n",
    "\n",
    "    split = full_split.split('-')[0]\n",
    "\n",
    "    if 'train' in full_split:\n",
    "        input_data_d['hard_train_data'], input_data_d['rand_train_data'] = sample_hard_rand(input_data_d['train_data'], svar)\n",
    "\n",
    "    split_path = full_split if 'train' not in full_split else ('hard_train' if 'Constant' in full_split else 'rand_train' )\n",
    "    this_input_data = input_data_d[split_path+'_data']\n",
    "\n",
    "    slSer = pd.Series( [ len(word_tokenize(s)) for s in this_input_data['text'] ], name='tok_len' )\n",
    "\n",
    "    jdf = pd.concat( [this_input_data, slSer], axis = 1 )\n",
    "    jdf = jdf.drop( 'text', axis=1 )\n",
    "    sub_jdf = jdf[['ID', 'tok_len']]\n",
    "    sub_jdf.loc[:, 'split_full'] = full_split\n",
    "    sub_jdf.loc[:, 'score_var'] = svar\n",
    "    sub_jdf.loc[:, 'split'] = [ s.split('-')[0] for s in sub_jdf['split_full'] ]\n",
    "\n",
    "    sub_ledg = ledger_diff[((ledger_diff['score_var']==svar) &  (ledger_diff['split_full']==full_split))]\n",
    "    this_join = pd.merge( sub_ledg, sub_jdf, on=['score_var', 'split_full', 'split', 'ID'], how='left' )\n",
    "    sub_join = this_join[this_join['tok_len'].isna()==False]\n",
    "\n",
    "    join_check_num = check_counts[( (check_counts['score_var']==svar) & (check_counts['split_full']==full_split) )]['count'].iloc[0]\n",
    "    if join_check_num != len(sub_join):\n",
    "        print('-----------------------------')\n",
    "        print(f'JOIN PROBLEM WITH {svar}-{full_split}')\n",
    "        print('-----------------------------')\n",
    "        raise ValueError('Shape of joined data must match expected dimensions of ledger_diff')\n",
    "    # print('passed join check...')\n",
    "\n",
    "    return sub_join\n",
    "\n",
    "\n",
    "\n",
    "USE_STORED = False\n",
    "\n",
    "if USE_STORED:\n",
    "    ledger_len = pd.read_csv( '/Users/ryancook/Downloads/these_ledgers/ledger_len.csv', keep_default_na=False )\n",
    "else:\n",
    "    # NOTE; these are older functions to avoid filtering NAs from demographic information\n",
    "    ## we are only interested in IDs / text strings here\n",
    "    from utils.data_loading_no_demo import *\n",
    "    from utils.data_processing import *\n",
    "    \n",
    "    #process files in parallel\n",
    "    import multiprocess\n",
    "    #multisetup\n",
    "    cores_to_use = multiprocess.cpu_count()-1\n",
    "    pool = multiprocess.Pool(cores_to_use)\n",
    "\n",
    "    from itertools import product\n",
    "    these_svars, these_full_splits = ledger_diff['score_var'].unique(), ledger_diff['split_full'].unique()\n",
    "    all_sv_splits = list( product( these_svars, these_full_splits ) )\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    with multiprocess.Pool(cores_to_use) as pool:\n",
    "        out_list = pool.map(proc_sl_sv_split, all_sv_splits)\n",
    "\n",
    "    ledger_len = pd.concat( out_list )\n",
    "    ledger_len = ledger_len.rename(columns={'ep_times_forgotten': 'times_forgotten'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger_len.to_csv( 'data/ledger_len.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muadib2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
